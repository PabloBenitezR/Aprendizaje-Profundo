{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibranfp/CursoAprendizajeProfundo/blob/2026-1/notebooks/1b_retropropagacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Autor:* Pablo Uriel Benítez Ramírez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V83__FrBij1f"
      },
      "source": [
        "# Retropropagación en red densa\n",
        "\n",
        "En este *notebook* se programa el algoritmo de retropropagación usando NumPy para una tarea de clasificación binaria presuponiendo una red densa con tres capas ocultas y una conexión residual que va de la salida de la primera capa oculta a la salida de la tercera capa oculta. \n",
        "\n",
        "Las neuronas de las capas ocultas cuentan con una función de activación \n",
        "\n",
        "$$\n",
        "ReLU(z) = \\max (0, z). \n",
        "$$\n",
        "Por su parte, la capa de salida\n",
        "está compuesta por una sola neurona sigmoide $σ(z) = 1/(1+e^{−z})$ . \n",
        "Para el entrenamiento se minimiza el promedio de la función de pérdida de entropía cruzada binaria\n",
        "$$\n",
        "ECB(\\textbf{y,$\\hat{\\textbf{y}}$})= -\\frac{1}{n}\\sum_{i=1}^n\\left[y^{(i)}\\log\\left(\\hat{y}^{(i)}\\right)+\\left(1-y^{(i)}\\right)\\log\\left(1-y^{(i)}\\right)\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSlnjW4Oi-FP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import floor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iAUmKI5jNuX"
      },
      "source": [
        "## Activación, pérdida y métricas\n",
        "La red neuronal densa está compuesta por una capa de 2 entradas ($x_1$ y $x_2$), tres capas ocultas con 10 neuronas con función de activación ReLU. Esta función de activación se define como:\n",
        "\n",
        "$$\n",
        "ReLU(z) = \\max (0, z). \n",
        "$$\n",
        "\n",
        "Y una capa de salida con una sola neurona con función de activación sigmoide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYhT3i68jf6x"
      },
      "outputs": [],
      "source": [
        "def relu(z):\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "def sigmoide(z):\n",
        "  return 1 / (1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx6SyrPhWBrw"
      },
      "source": [
        "La derivada de la función ReLU es,\n",
        "\n",
        "$$\n",
        "\\frac{\\partial ReLU (z)}{\\partial z} = \\begin{cases}\n",
        "1 & \\text{si }~z>0\\\\\n",
        "0 & \\text{si }~z\\leq 0\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJxvxKeAjn24"
      },
      "outputs": [],
      "source": [
        "def derivada_relu(z):\n",
        "  return (z > 0).astype(z.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WxI8FfLXKHv"
      },
      "source": [
        "Podemos ver la operación XOR como una tarea de clasificación binaria a partir de 2 entradas. Por lo tanto, usaremos la función de pérdida de entropía cruzada binaria:\n",
        "\n",
        "$$\n",
        "ECB(\\mathbf{y}, \\mathbf{\\hat{y}})  = -\\frac{1}{n}\\sum_{i=1}^N \\left[ y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDjlmpAQjR3X"
      },
      "outputs": [],
      "source": [
        "def entropia_cruzada_binaria(y, p):\n",
        "  p[p == 0] = np.nextafter(0., 1.)\n",
        "  p[p == 1] = np.nextafter(1., 0.)\n",
        "  return -(np.log(p[y == 1]).sum() + np.log(1 - p[y == 0]).sum())/y.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8nMdK-RYWMS"
      },
      "source": [
        "Asimismo, calcularemos la exactitud para medir el rendimiento del modelo aprendido por la red neuronal densa:\n",
        "\n",
        "$$\n",
        "exactitud = \\frac{correctos}{total}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wxvZq10jIM3"
      },
      "outputs": [],
      "source": [
        "def exactitud(y, y_predicha):\n",
        "  return (y == y_predicha).mean() * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p02hAdUFZNLL"
      },
      "source": [
        "Ahora, se define la función que propaga hacia adelante una entrada $\\mathbf{X} \\in \\mathbb{R}^{m \\times d}$.\n",
        "\n",
        " Como la red está compuesta de 4 capas densas (3 oculta y 1 de salida), tenemos 4 matrices de pesos con sus correspondientes vectores de sesgos\n",
        " \n",
        "  $\\{\\mathbf{W}^{\\{1\\}} \\in \\mathbb{R}^{d \\times l}, \\mathbf{b}^{\\{1\\}} \\in \\mathbb{R}^{l \\times 1}\\}$ \n",
        "  \n",
        "   $\\{\\mathbf{W}^{\\{2\\}} \\in \\mathbb{R}^{l \\times k}, \\mathbf{b}^{\\{2\\}} \\in \\mathbb{R}^{k \\times 1}\\}$ \n",
        "\n",
        "   $\\{\\mathbf{W}^{\\{3\\}} \\in \\mathbb{R}^{l \\times k}, \\mathbf{b}^{\\{3\\}} \\in \\mathbb{R}^{k \\times 1}\\}$ \n",
        "\n",
        "   $\\{\\mathbf{W}^{\\{4\\}} \\in \\mathbb{R}^{l \\times k}, \\mathbf{b}^{\\{4\\}} \\in \\mathbb{R}^{k \\times 1}\\}$ \n",
        "   \n",
        "   de las capas ocultas y la capa de salida respectivamente. Así, podemos llevar a cabo la propagación hacia adelante en esta red de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\t\\begin{split}\n",
        "\t\t\t\t\\mathbf{A}^{\\{1\\}} & =  \\mathbf{X} \\\\\n",
        "\t\t\t\t\\mathbf{Z}^{\\{2\\}} & =  \\mathbf{A}^{\\{1\\}} \\cdot \\mathbf{W}^{\\{1\\}} + \\mathbf{b}^{\\{1\\}}\\\\\n",
        "\t\t\t\t\\mathbf{A}^{\\{2\\}} & =  ReLU(\\mathbf{Z}^{\\{2\\}}) \\\\\n",
        "\t\t\t\t\\mathbf{Z}^{\\{3\\}} & =  \\mathbf{A}^{\\{2\\}} \\cdot \\mathbf{W}^{\\{2\\}}  + \\mathbf{b}^{\\{2\\}}\\\\\n",
        "\t\t\t\t\\mathbf{A}^{\\{3\\}} & =  ReLU(\\mathbf{Z}^{\\{3\\}})\\\\\n",
        "\t\t\t\t\\mathbf{Z}^{\\{4\\}} & =  \\mathbf{A}^{\\{3\\}} \\cdot \\mathbf{W}^{\\{3\\}}  + \\mathbf{b}^{\\{3\\}}+\\mathbf{A}^{\\{2\\}}\\\\\n",
        "\t\t\t\t\\mathbf{A}^{\\{4\\}} & =  ReLU(\\mathbf{Z}^{\\{4\\}})\\\\\n",
        "\t\t\t\t\\mathbf{Z}^{\\{5\\}} & =  \\mathbf{A}^{\\{4\\}} \\cdot \\mathbf{W}^{\\{4\\}}  + \\mathbf{b}^{\\{4\\}}\\\\\n",
        "\t\t\t\t\\mathbf{A}^{\\{5\\}} & =  \\sigma(\\mathbf{Z}^{\\{5\\}})\\\\\n",
        "\t\t\t\t\\mathbf{\\hat{y}} & =  \\mathbf{A}^{\\{5\\}}\n",
        "\t\t\\end{split}\n",
        "  $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAsEk-zajvpX"
      },
      "outputs": [],
      "source": [
        "def hacia_adelante(X, W1, b1, W2, b2, W3, b3, W4, b4):\n",
        "\n",
        "  \n",
        "  \n",
        "  Z2 = X @ W1 + b1\n",
        "  A2 = relu(Z2)\n",
        "\n",
        "  Z3 = A2 @ W2 + b2\n",
        "  A3 = relu(Z3)\n",
        "\n",
        "  Z4 = A3 @ W3 + b3 + A2\n",
        "  A4 = relu(Z4)\n",
        "\n",
        "  Z5 = A4 @ W4 + b4\n",
        "  y_hat = sigmoide(Z5)\n",
        "  \n",
        "\n",
        "\n",
        "  return Z2, A2, Z3, A3, Z4, A4, Z5, y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytocKK_73wxZ"
      },
      "source": [
        "Para facilitar el entrenamiento definimos una función que ordene aleatoriamente y vaya generando lotes del conjunto completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZsUKc4N0QSp"
      },
      "outputs": [],
      "source": [
        "def genera_lotes(X, y, t_lote=16):\n",
        "  n_lotes = floor(X.shape[0] / t_lote) # se calcula el número de lotes (si el\n",
        "                                       # tamaño del último lote < t_lote se\n",
        "                                       # ignora\n",
        "\n",
        "  perm = np.random.permutation(X.shape[0]) # se genera una permutación aleatoria\n",
        "  Xperm = X[perm] # se reordenan las entradas y\n",
        "  yperm = y[perm] # las entradas usando la permutación\n",
        "  for lote in range(n_lotes): # se van tomando t_lote entradas y salidas\n",
        "    Xlote = Xperm[lote * t_lote:(lote + 1) * t_lote]\n",
        "    ylote = yperm[lote * t_lote:(lote + 1) * t_lote]\n",
        "    yield Xlote, ylote"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiOT6jqXjzwQ"
      },
      "source": [
        "## Descenso por gradiente mediante retropropagación de errores\n",
        "Se define la función para entrenar nuestra red neuronal usando descenso por gradiente. Para calcular el gradiente de la función de pérdida respecto a los pesos y sesgos en cada capa empleamos el algoritmo de retropropagación. Para este caso, serían las siguientes expresiones.\n",
        "\n",
        "Dado \n",
        "$$\n",
        "\\begin{align*}\n",
        "Z_2&=X\\cdot W_1+b_1,~~~~~~~~~~~~~~~A_2=ReLU(Z_2),\\\\\n",
        "Z_3&=A_2\\cdot W_2+b_2,~~~~~~~~~~~~~~~A_3=ReLU(Z_3),\\\\\n",
        "Z_4&=A_3\\cdot W_3+b_3+A_2,~~~~~~A_4=ReLU(Z_4),\\\\\n",
        "Z_5&=A_4\\cdot W_4+b_4,~~~~~~~~~~~~~~\\hat{y}=\\sigma(Z_5),\n",
        "\\end{align*}\n",
        "$$\n",
        "Sea \n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\boldsymbol{\\delta}^{\\{5\\}} & =  \\mathbf{\\hat{y}}^{(i)} - \\mathbf{y}^{(i)}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Capa de salida $(W_4,b_4)$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{W}^{\\{4\\}}} & =  \\mathbf{A}^{\\{4\\}\\top} \\cdot \\boldsymbol{\\delta}^{\\{5\\}}\\\\\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{b}^{\\{4\\}}} & =  \\sum_{j=1}^{k}\\boldsymbol{\\delta}^{\\{5\\}}_{j, :}\\\\\n",
        "\\boldsymbol{\\delta}^{\\{4\\}} & =  (\\boldsymbol{\\delta}^{\\{5\\}}\\cdot \\mathbf{W}^{\\{4\\}\\top}) \\odot \\frac{\\partial \\mathbf{A}^{\\{4\\}}}{\\partial \\mathbf{Z}^{\\{4\\}}}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "tercera capa oculta $(W_3,b_3)$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{W}^{\\{3\\}}} & =  \\mathbf{A}^{\\{3\\}\\top} \\cdot \\boldsymbol{\\delta}^{\\{4\\}}\\\\\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{b}^{\\{3\\}}} & =  \\sum_{j=1}^{k}\\boldsymbol{\\delta}^{\\{4\\}}_{j, :}\\\\\n",
        "\\boldsymbol{\\delta}^{\\{3\\}} & =  (\\boldsymbol{\\delta}^{\\{4\\}}\\cdot \\mathbf{W}^{\\{3\\}\\top}) \\odot \\frac{\\partial \\mathbf{A}^{\\{3\\}}}{\\partial \\mathbf{Z}^{\\{3\\}}}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "segunda capa oculta $(W_2,b_2)$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{W}^{\\{2\\}}} & =  \\mathbf{A}^{\\{2\\}\\top} \\cdot \\boldsymbol{\\delta}^{\\{3\\}}\\\\\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{b}^{\\{2\\}}} & =  \\sum_{j=1}^{k}\\boldsymbol{\\delta}^{\\{3\\}}_{j, :}\\\\\n",
        "\\boldsymbol{\\delta}^{\\{2\\}} & =  (\\boldsymbol{\\delta}^{\\{3\\}}\\cdot \\mathbf{W}^{\\{2\\}\\top}+\\delta_4) \\odot \\frac{\\partial \\mathbf{A}^{\\{2\\}}}{\\partial \\mathbf{Z}^{\\{2\\}}}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "primera capa oculta $(W_1,b_1)$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{W}^{\\{1\\}}} & =  \\mathbf{X}^{\\{1\\}\\top} \\cdot \\boldsymbol{\\delta}^{\\{2\\}}\\\\\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{b}^{\\{1\\}}} & =  \\sum_{j=1}^{k}\\boldsymbol{\\delta}^{\\{2\\}}_{j, :}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P7i6eLgkJdg"
      },
      "outputs": [],
      "source": [
        "def retropropagacion(X, y, alpha = 0.01, t_lote=16, n_epocas = 100, n_ocultas = 10):\n",
        "  n_ejemplos = X.shape[0]\n",
        "  n_entradas = X.shape[1]\n",
        "  n_lotes = floor(X.shape[0] / t_lote) # se calcula el número de lotes (si el\n",
        "                                       # tamaño del último lote < t_lote se\n",
        "                                       # ignora\n",
        "\n",
        "  # Inicialización de las matrices de pesos W y V\n",
        "  # capas ocultas\n",
        "\n",
        "  h= n_ocultas\n",
        "\n",
        "  W1 = np.sqrt(1.0 / n_entradas) * np.random.randn(n_entradas, n_ocultas)\n",
        "  b1 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  W2 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, n_ocultas)\n",
        "  b2 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  W3 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, n_ocultas)\n",
        "  b3 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  # capa de salida\n",
        "  W4 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, 1)\n",
        "  b4 = np.zeros((1, 1))\n",
        "\n",
        "  perdidas = np.zeros((n_epocas))\n",
        "  exactitudes = np.zeros((n_epocas))\n",
        "  y_predicha = np.zeros((y.shape))\n",
        "\n",
        "  for i in range(n_epocas):\n",
        "    for Xlote,ylote in genera_lotes(X, y, t_lote=t_lote):\n",
        "      Z2, A2, Z3, A3, Z4, A4, Z5, y_hat = hacia_adelante(Xlote, W1, b1, W2, b2, W3, b3, W4, b4)\n",
        "\n",
        "      # cálculo de gradientes para W4 y b4 por retropropagación \n",
        "      # (capa de salida)\n",
        "      dZ5 = y_hat - ylote/ylote.shape[0]\n",
        "      dW4 = A4.T @ dZ5\n",
        "      db4 = dZ5.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W3 y b3 por retropropagación \n",
        "      # (tercera capa oculta)\n",
        "      dA4 = dZ5 @ W4.T\n",
        "      dZ4 = dA4 * derivada_relu(Z4)\n",
        "      dW3 = A3.T @ dZ4\n",
        "      db3 = dZ4.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W2 y b2 por retropropagación \n",
        "      # (segunda capa oculta)\n",
        "      dA3 = dZ4 @ W3.T\n",
        "      dZ3 = dA3 * derivada_relu(Z3)\n",
        "      dW2 = A2.T @ dZ3\n",
        "      db2 = dZ3.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W1 y b1 por retropropagación\n",
        "      # (primera capa oculta)\n",
        "      dA2 = (dZ3 @ W2.T) + dZ4\n",
        "      dZ2 = dA2 * derivada_relu(Z2)\n",
        "      dW1 = Xlote.T @ dZ2\n",
        "      db1 = dZ2.sum(axis=0)\n",
        "\n",
        "      ####################################\n",
        "      # IMPORTANTE\n",
        "      # la actualización de los parámetros\n",
        "      # debe hacerse de forma simultánea\n",
        "      W4 -= alpha * dW4; b4 -= alpha * db4\n",
        "      W3 -= alpha * dW3; b3 -= alpha * db3\n",
        "      W2 -= alpha * dW2; b2 -= alpha * db2\n",
        "      W1 -= alpha * dW1; b1 -= alpha * db1\n",
        "\n",
        "      # calcula la pérdida en la época\n",
        "      perdidas[i] += entropia_cruzada_binaria(ylote, y_hat)\n",
        "      exactitudes[i] += exactitud(ylote, np.round(y_hat))\n",
        "\n",
        "    perdidas[i] /= n_lotes\n",
        "    exactitudes[i] /= n_lotes\n",
        "    print(f'Epoch {i}: Pérdida = {perdidas[i]} Exactitud = {exactitudes[i]}')\n",
        "\n",
        "  return W1, b1, W2, b2, W3, b3, W4, b4, perdidas, exactitudes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nau0HWsrkRxg"
      },
      "source": [
        "## Evaluación preliminar con XOR\n",
        "Para probar nuestra red, generamos los ejemplos correspondientes a la operación XOR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8txXZ34GkUAF"
      },
      "outputs": [],
      "source": [
        "# ejemplo (XOR)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0, 1, 1, 0]]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLT8avfhkYH7"
      },
      "source": [
        "Entrenamos nuestra red con estos ejemplos por 500 épocas usando una tasa de aprendizaje $\\alpha = 0.25$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijKxVwZ3kbyR",
        "outputId": "7fa7af7b-5f4f-44e1-f026-655bb27d468f"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "W1, b1, W2, b2, W3, b3, W4, b4, perdidas, exactitudes = retropropagacion(X,\n",
        "                                                         y,\n",
        "                                                         alpha = 0.25,\n",
        "                                                         n_epocas = 500,\n",
        "                                                         n_ocultas = 10,\n",
        "                                                         t_lote=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8A3KZ5JkDJ3"
      },
      "source": [
        "Graficamos el valor de la pérdida y la exactitud en cada época para ver el comportamiento de nuestra red durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "yglJSF9nkR7k",
        "outputId": "305c9e75-1881-4489-b91a-b44181b4e645"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(perdidas.size), perdidas, label='ECB')\n",
        "plt.plot(np.arange(exactitudes.size), exactitudes, label='Exactitud')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu9XpSks3lGx"
      },
      "source": [
        "Nota: solo llega al 75% si se define  $\\alpha=0.5$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCIdCyofjzO5"
      },
      "source": [
        "## 1. Clasificación no lineal\n",
        "Ahora se entrena y evalúa la red mediante descenso por gradiente y el algoritmo de retropropagación de errores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u0IW7bk5Vzx"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "Xreal = np.random.randn(1000, 2)\n",
        "yreal = np.logical_xor(Xreal[:, 0] > 0, Xreal[:, 1] > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT7VJ03_5DuC"
      },
      "source": [
        "Dividimos el conjunto de datos generado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ7LV2bl5Gt6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xent, Xval, yent, yval = train_test_split(Xreal, yreal, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cn0Z74s5WbE"
      },
      "source": [
        "Definimos una función para graficar el conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oAZNyyFmcMA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def grafica_datos(modelo, X_ent, y_ent, X_val, y_val, frontera=None):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.scatter(X_ent[:, 0], X_ent[:, 1], c=y_ent, cmap=plt.cm.coolwarm,\n",
        "             s=30, edgecolors='k', label='Validación')\n",
        "  ax.scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=plt.cm.coolwarm,\n",
        "             s=30, alpha=0.5, edgecolors='k', label='Validación')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "bAfChaH4PMI_",
        "outputId": "c1885809-9bdd-490c-bd7b-4fba501b384e"
      },
      "outputs": [],
      "source": [
        "grafica_datos(_, Xent, yent, Xval, yval, frontera=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-akq1yGk4pRm"
      },
      "source": [
        "Finalmente, entrenamos nuestra red con estos ejemplos por 500 épocas usando una tasa de aprendizaje $\\alpha = 0.05$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wygfjVm-4pRm",
        "outputId": "8cb7b329-a0a3-451d-ceba-f17734b9c532"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "W1, b1, W2, b2, W3, b3, W4, b4, perdidas, exactitudes = retropropagacion(Xent,\n",
        "                                                         yent[:, np.newaxis],\n",
        "                                                         alpha = 0.05,\n",
        "                                                         t_lote=16,\n",
        "                                                         n_epocas = 500,\n",
        "                                                         n_ocultas = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thvIIaQd4pRn"
      },
      "source": [
        "Graficamos el valor de la pérdida y la exactitud en cada época para ver el comportamiento de nuestra red durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "oAmd1Q9a4pRn",
        "outputId": "bc13e7f3-a48c-4a83-c8de-2d3aaf122023"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(perdidas.size), perdidas, label='ECB')\n",
        "plt.plot(np.arange(exactitudes.size), exactitudes, label='Exactitud')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPpQ_WNS6egy"
      },
      "source": [
        "Evalúamos el desempeño del modelo en el conjunto de validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdK1o_Ho6bpS",
        "outputId": "7bae07c0-121f-4158-a26a-37e7e9777cf0"
      },
      "outputs": [],
      "source": [
        "_, _, _,_,_,_,_, y_hat = hacia_adelante(Xval, W1, b1, W2, b2, W3, b3, W4, b4)\n",
        "print(exactitud(yval[:, np.newaxis], np.round(y_hat).astype(bool)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparación de resultados sin residual y con sigmoide\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def derivada_sigmoide(x):\n",
        "    s = sigmoide(x)\n",
        "    return s * (1 - s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\begin{align*}\n",
        "\t\t\t\t\\mathbf{A}^{\\{1\\}} & =  \\mathbf{X} \\\\\n",
        "\t\t\t\t\\mathbf{Z}^{\\{2\\}} & =  \\mathbf{A}^{\\{1\\}} \\cdot \\mathbf{W}^{\\{1\\}} + \\mathbf{b}^{\\{1\\}}\\\\\n",
        "\t\t\t\t\\mathbf{A}^{\\{2\\}} & =  \\sigma(\\mathbf{Z}^{\\{2\\}}) \\\\\n",
        "\t\t\t\t\\mathbf{Z}^{\\{3\\}} & =  \\mathbf{A}^{\\{2\\}} \\cdot \\mathbf{W}^{\\{2\\}}  + \\mathbf{b}^{\\{2\\}}\\\\\n",
        "\t\t\t\t\\mathbf{A}^{\\{3\\}} & =  \\sigma(\\mathbf{Z}^{\\{3\\}})\\\\\n",
        "\t\t\t\t\\mathbf{Z}^{\\{4\\}} & =  \\mathbf{A}^{\\{3\\}} \\cdot \\mathbf{W}^{\\{3\\}}  + \\mathbf{b}^{\\{3\\}}\\\\\n",
        "\t\t\t\t\\mathbf{A}^{\\{4\\}} & =  \\sigma(\\mathbf{Z}^{\\{4\\}})\\\\\n",
        "\t\t\t\t\\mathbf{Z}^{\\{5\\}} & =  \\mathbf{A}^{\\{4\\}} \\cdot \\mathbf{W}^{\\{4\\}}  + \\mathbf{b}^{\\{4\\}}\\\\\n",
        "\t\t\t\t\\mathbf{A}^{\\{5\\}} & =  \\sigma(\\mathbf{Z}^{\\{5\\}})\\\\\n",
        "\t\t\t\t\\mathbf{\\hat{y}} & =  \\mathbf{A}^{\\{5\\}}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hacia_adelante_nr(X, W1, b1, W2, b2, W3, b3, W4, b4):\n",
        "\n",
        "  Z2 = X @ W1 + b1\n",
        "  A2 = sigmoide(Z2)\n",
        "\n",
        "  Z3 = A2 @ W2 + b2\n",
        "  A3 = sigmoide(Z3)\n",
        "\n",
        "  Z4 = A3 @ W3 + b3 # se elimina el residual\n",
        "  A4 = sigmoide(Z4)\n",
        "\n",
        "  Z5 = A4 @ W4 + b4\n",
        "  y_hat = sigmoide(Z5)\n",
        "  \n",
        "\n",
        "  return Z2, A2, Z3, A3, Z4, A4, Z5, y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Descenso por gradiente \n",
        "Se define la función para entrenar nuestra red neuronal usando descenso por gradiente. Para calcular el gradiente de la función de pérdida respecto a los pesos y sesgos en cada capa empleamos el algoritmo de retropropagación. Para este caso, serían las siguientes expresiones.\n",
        "\n",
        "Dado \n",
        "$$\n",
        "\\begin{align*}\n",
        "Z_2&=X\\cdot W_1+b_1,~~~~~~~~~~~~~~~~A_2=\\sigma(Z_2),\\\\\n",
        "Z_3&=A_2\\cdot W_2+b_2,~~~~~~~~~~~~~~~A_3=\\sigma(Z_3),\\\\\n",
        "Z_4&=A_3\\cdot W_3+b_3,~~~~~~~~~~~~~~~A_4=\\sigma(Z_4),\\\\\n",
        "Z_5&=A_4\\cdot W_4+b_4,~~~~~~~~~~~~~~~~~~\\hat{y}=\\sigma(Z_5),\n",
        "\\end{align*}\n",
        "$$\n",
        "Sea\n",
        "$$ \n",
        "\\begin{align*}\n",
        "\\boldsymbol{\\delta}^{\\{5\\}} & =  \\mathbf{\\hat{y}}^{(i)} - \\mathbf{y}^{(i)}\n",
        "\\end{align*}\n",
        "$$\n",
        "Capa de salida $(W_4,b_4)$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{W}^{\\{4\\}}} & =  \\mathbf{A}^{\\{4\\}\\top} \\cdot \\boldsymbol{\\delta}^{\\{5\\}}\\\\\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{b}^{\\{4\\}}} & =  \\sum_{j=1}^{k}\\boldsymbol{\\delta}^{\\{5\\}}_{j, :}\\\\\n",
        "\\boldsymbol{\\delta}^{\\{4\\}} & =  (\\boldsymbol{\\delta}^{\\{5\\}}\\cdot \\mathbf{W}^{\\{4\\}\\top}) \\odot \\frac{\\partial \\mathbf{A}^{\\{4\\}}}{\\partial \\mathbf{Z}^{\\{4\\}}}\n",
        "\\end{align*}\n",
        "$$\n",
        "Tercera capa oculta $(W_3,b_3)$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{W}^{\\{3\\}}} & =  \\mathbf{A}^{\\{3\\}\\top} \\cdot \\boldsymbol{\\delta}^{\\{4\\}}\\\\\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{b}^{\\{3\\}}} & =  \\sum_{j=1}^{k}\\boldsymbol{\\delta}^{\\{4\\}}_{j, :}\\\\\n",
        "\\boldsymbol{\\delta}^{\\{3\\}} & =  (\\boldsymbol{\\delta}^{\\{4\\}}\\cdot \\mathbf{W}^{\\{3\\}\\top}) \\odot \\frac{\\partial \\mathbf{A}^{\\{3\\}}}{\\partial \\mathbf{Z}^{\\{3\\}}}\n",
        "\\end{align*}\n",
        "$$\n",
        "segunda capa oculta $(W_2,b_2)$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{W}^{\\{2\\}}} & =  \\mathbf{A}^{\\{2\\}\\top} \\cdot \\boldsymbol{\\delta}^{\\{3\\}}\\\\\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{b}^{\\{2\\}}} & =  \\sum_{j=1}^{k}\\boldsymbol{\\delta}^{\\{3\\}}_{j, :}\\\\\n",
        "\\boldsymbol{\\delta}^{\\{2\\}} & =  (\\boldsymbol{\\delta}^{\\{3\\}}\\cdot \\mathbf{W}^{\\{2\\}\\top}) \\odot \\frac{\\partial \\mathbf{A}^{\\{2\\}}}{\\partial \\mathbf{Z}^{\\{2\\}}}\n",
        "\\end{align*}\n",
        "$$\n",
        "primera capa oculta $(W_1,b_1)$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{W}^{\\{1\\}}} & =  \\mathbf{X}^{\\{1\\}\\top} \\cdot \\boldsymbol{\\delta}^{\\{2\\}}\\\\\n",
        "\\frac{\\partial ECB(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{b}^{\\{1\\}}} & =  \\sum_{j=1}^{k}\\boldsymbol{\\delta}^{\\{2\\}}_{j, :}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retropropagacion_nr(X, y, alpha = 0.01, t_lote=16, n_epocas = 100, n_ocultas = 10):\n",
        "  n_ejemplos = X.shape[0]\n",
        "  n_entradas = X.shape[1]\n",
        "  n_lotes = floor(X.shape[0] / t_lote) # se calcula el número de lotes (si el\n",
        "                                       # tamaño del último lote < t_lote se\n",
        "                                       # ignora\n",
        "\n",
        "  # Inicialización de las matrices de pesos W y V\n",
        "  # capas ocultas\n",
        "\n",
        "  h= n_ocultas\n",
        "\n",
        "  W1 = np.sqrt(1.0 / n_entradas) * np.random.randn(n_entradas, n_ocultas)\n",
        "  b1 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  W2 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, n_ocultas)\n",
        "  b2 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  W3 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, n_ocultas)\n",
        "  b3 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  # capa de salida\n",
        "  W4 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, 1)\n",
        "  b4 = np.zeros((1, 1))\n",
        "\n",
        "  perdidas = np.zeros((n_epocas))\n",
        "  exactitudes = np.zeros((n_epocas))\n",
        "  y_predicha = np.zeros((y.shape))\n",
        "\n",
        "  for i in range(n_epocas):\n",
        "    for Xlote,ylote in genera_lotes(X, y, t_lote=t_lote):\n",
        "      Z2, A2, Z3, A3, Z4, A4, Z5, y_hat = hacia_adelante_nr(Xlote, W1, b1, W2, b2, W3, b3, W4, b4)\n",
        "\n",
        "      # cálculo de gradientes para W4 y b4 por retropropagación \n",
        "      # (capa de salida)\n",
        "      dZ5 = y_hat - ylote/ylote.shape[0]\n",
        "      dW4 = A4.T @ dZ5\n",
        "      db4 = dZ5.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W3 y b3 por retropropagación \n",
        "      # (tercera capa oculta)\n",
        "      dA4 = dZ5 @ W4.T\n",
        "      dZ4 = dA4 * derivada_sigmoide(Z4)\n",
        "      dW3 = A3.T @ dZ4\n",
        "      db3 = dZ4.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W2 y b2 por retropropagación \n",
        "      # (segunda capa oculta)\n",
        "      dA3 = dZ4 @ W3.T\n",
        "      dZ3 = dA3 * derivada_sigmoide(Z3)\n",
        "      dW2 = A2.T @ dZ3\n",
        "      db2 = dZ3.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W1 y b1 por retropropagación\n",
        "      # (primera capa oculta)\n",
        "      dA2 = (dZ3 @ W2.T) \n",
        "      dZ2 = dA2 * derivada_sigmoide(Z2)\n",
        "      dW1 = Xlote.T @ dZ2\n",
        "      db1 = dZ2.sum(axis=0)\n",
        "\n",
        "      ####################################\n",
        "      # IMPORTANTE\n",
        "      # la actualización de los parámetros\n",
        "      # debe hacerse de forma simultánea\n",
        "      W4 -= alpha * dW4; b4 -= alpha * db4\n",
        "      W3 -= alpha * dW3; b3 -= alpha * db3\n",
        "      W2 -= alpha * dW2; b2 -= alpha * db2\n",
        "      W1 -= alpha * dW1; b1 -= alpha * db1\n",
        "\n",
        "      # calcula la pérdida en la época\n",
        "      perdidas[i] += entropia_cruzada_binaria(ylote, y_hat)\n",
        "      exactitudes[i] += exactitud(ylote, np.round(y_hat))\n",
        "\n",
        "    perdidas[i] /= n_lotes\n",
        "    exactitudes[i] /= n_lotes\n",
        "    print(f'Epoch {i}: Pérdida = {perdidas[i]} Exactitud = {exactitudes[i]}')\n",
        "\n",
        "  return W1, b1, W2, b2, W3, b3, W4, b4, perdidas, exactitudes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente, entrenamos nuestra red con estos ejemplos por 500 épocas usando una tasa de aprendizaje $\\alpha = 0.01$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "W1, b1, W2, b2, W3, b3, W4, b4, perdidas, exactitudes = retropropagacion_nr(Xent,\n",
        "                                                         yent[:, np.newaxis],\n",
        "                                                         alpha = 0.01,\n",
        "                                                         t_lote=16,\n",
        "                                                         n_epocas = 500,\n",
        "                                                         n_ocultas = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.arange(perdidas.size), perdidas, label='ECB')\n",
        "plt.plot(np.arange(exactitudes.size), exactitudes, label='Exactitud')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluamos el desempeño del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_, _, _,_,_,_,_, y_hat = hacia_adelante_nr(Xval, W1, b1, W2, b2, W3, b3, W4, b4)\n",
        "print(exactitud(yval[:, np.newaxis], np.round(y_hat).astype(bool)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estudia el efecto en el entrenamiento de no usar el promedio en la función ECB esto es\n",
        "$$\n",
        "    \\begin{align*}\n",
        "        ECB(\\textbf{y,$\\hat{\\textbf{y}}$})= -\\sum_{i=1}^n\\left[y^{(i)}\\log\\left(\\hat{y}^{(i)}\\right)+\\left(1-y^{(i)}\\right)\\log\\left(1-y^{(i)}\\right)\\right]\n",
        "    \\end{align*}\n",
        "    $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retropropagacion_snp(X, y, alpha = 0.01, t_lote=16, n_epocas = 100, n_ocultas = 10):\n",
        "  n_ejemplos = X.shape[0]\n",
        "  n_entradas = X.shape[1]\n",
        "  n_lotes = floor(X.shape[0] / t_lote) # se calcula el número de lotes (si el\n",
        "                                       # tamaño del último lote < t_lote se\n",
        "                                       # ignora\n",
        "\n",
        "  # Inicialización de las matrices de pesos W y V\n",
        "  # capas ocultas\n",
        "\n",
        "  h= n_ocultas\n",
        "\n",
        "  W1 = np.sqrt(1.0 / n_entradas) * np.random.randn(n_entradas, n_ocultas)\n",
        "  b1 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  W2 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, n_ocultas)\n",
        "  b2 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  W3 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, n_ocultas)\n",
        "  b3 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  # capa de salida\n",
        "  W4 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, 1)\n",
        "  b4 = np.zeros((1, 1))\n",
        "\n",
        "  perdidas = np.zeros((n_epocas))\n",
        "  exactitudes = np.zeros((n_epocas))\n",
        "  y_predicha = np.zeros((y.shape))\n",
        "\n",
        "  for i in range(n_epocas):\n",
        "    for Xlote,ylote in genera_lotes(X, y, t_lote=t_lote):\n",
        "      Z2, A2, Z3, A3, Z4, A4, Z5, y_hat = hacia_adelante(Xlote, W1, b1, W2, b2, W3, b3, W4, b4)\n",
        "\n",
        "      # cálculo de gradientes para W4 y b4 por retropropagación \n",
        "      # (capa de salida)\n",
        "      dZ5 = y_hat - ylote\n",
        "      dW4 = A4.T @ dZ5\n",
        "      db4 = dZ5.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W3 y b3 por retropropagación \n",
        "      # (tercera capa oculta)\n",
        "      dA4 = dZ5 @ W4.T\n",
        "      dZ4 = dA4 * derivada_relu(Z4)\n",
        "      dW3 = A3.T @ dZ4\n",
        "      db3 = dZ4.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W2 y b2 por retropropagación \n",
        "      # (segunda capa oculta)\n",
        "      dA3 = dZ4 @ W3.T\n",
        "      dZ3 = dA3 * derivada_relu(Z3)\n",
        "      dW2 = A2.T @ dZ3\n",
        "      db2 = dZ3.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W1 y b1 por retropropagación\n",
        "      # (primera capa oculta)\n",
        "      dA2 = (dZ3 @ W2.T) + dZ4\n",
        "      dZ2 = dA2 * derivada_relu(Z2)\n",
        "      dW1 = Xlote.T @ dZ2\n",
        "      db1 = dZ2.sum(axis=0)\n",
        "\n",
        "      ####################################\n",
        "      # IMPORTANTE\n",
        "      # la actualización de los parámetros\n",
        "      # debe hacerse de forma simultánea\n",
        "      W4 -= alpha * dW4; b4 -= alpha * db4\n",
        "      W3 -= alpha * dW3; b3 -= alpha * db3\n",
        "      W2 -= alpha * dW2; b2 -= alpha * db2\n",
        "      W1 -= alpha * dW1; b1 -= alpha * db1\n",
        "\n",
        "      # calcula la pérdida en la época\n",
        "      perdidas[i] += entropia_cruzada_binaria(ylote, y_hat)\n",
        "      exactitudes[i] += exactitud(ylote, np.round(y_hat))\n",
        "\n",
        "    perdidas[i] /= n_lotes\n",
        "    exactitudes[i] /= n_lotes\n",
        "    print(f'Epoch {i}: Pérdida = {perdidas[i]} Exactitud = {exactitudes[i]}')\n",
        "\n",
        "  return W1, b1, W2, b2, W3, b3, W4, b4, perdidas, exactitudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "W1, b1, W2, b2, W3, b3, W4, b4, perdidas, exactitudes = retropropagacion_snp(Xent,\n",
        "                                                         yent[:, np.newaxis],\n",
        "                                                         alpha = 0.01,\n",
        "                                                         t_lote=16,\n",
        "                                                         n_epocas = 500,\n",
        "                                                         n_ocultas = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.arange(perdidas.size), perdidas, label='ECB')\n",
        "plt.plot(np.arange(exactitudes.size), exactitudes, label='Exactitud')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retropropagacion_nr_snp(X, y, alpha = 0.01, t_lote=16, n_epocas = 100, n_ocultas = 10):\n",
        "  n_ejemplos = X.shape[0]\n",
        "  n_entradas = X.shape[1]\n",
        "  n_lotes = floor(X.shape[0] / t_lote) # se calcula el número de lotes (si el\n",
        "                                       # tamaño del último lote < t_lote se\n",
        "                                       # ignora\n",
        "\n",
        "  # Inicialización de las matrices de pesos W y V\n",
        "  # capas ocultas\n",
        "\n",
        "  h= n_ocultas\n",
        "\n",
        "  W1 = np.sqrt(1.0 / n_entradas) * np.random.randn(n_entradas, n_ocultas)\n",
        "  b1 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  W2 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, n_ocultas)\n",
        "  b2 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  W3 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, n_ocultas)\n",
        "  b3 = np.zeros((1, n_ocultas))\n",
        "\n",
        "  # capa de salida\n",
        "  W4 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, 1)\n",
        "  b4 = np.zeros((1, 1))\n",
        "\n",
        "  perdidas = np.zeros((n_epocas))\n",
        "  exactitudes = np.zeros((n_epocas))\n",
        "  y_predicha = np.zeros((y.shape))\n",
        "\n",
        "  for i in range(n_epocas):\n",
        "    for Xlote,ylote in genera_lotes(X, y, t_lote=t_lote):\n",
        "      Z2, A2, Z3, A3, Z4, A4, Z5, y_hat = hacia_adelante_nr(Xlote, W1, b1, W2, b2, W3, b3, W4, b4)\n",
        "\n",
        "      # cálculo de gradientes para W4 y b4 por retropropagación \n",
        "      # (capa de salida)\n",
        "      dZ5 = y_hat - ylote\n",
        "      dW4 = A4.T @ dZ5\n",
        "      db4 = dZ5.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W3 y b3 por retropropagación \n",
        "      # (tercera capa oculta)\n",
        "      dA4 = dZ5 @ W4.T\n",
        "      dZ4 = dA4 * derivada_sigmoide(Z4)\n",
        "      dW3 = A3.T @ dZ4\n",
        "      db3 = dZ4.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W2 y b2 por retropropagación \n",
        "      # (segunda capa oculta)\n",
        "      dA3 = dZ4 @ W3.T\n",
        "      dZ3 = dA3 * derivada_sigmoide(Z3)\n",
        "      dW2 = A2.T @ dZ3\n",
        "      db2 = dZ3.sum(axis=0)\n",
        "\n",
        "      # cálculo de gradientes para W1 y b1 por retropropagación\n",
        "      # (primera capa oculta)\n",
        "      dA2 = (dZ3 @ W2.T) \n",
        "      dZ2 = dA2 * derivada_sigmoide(Z2)\n",
        "      dW1 = Xlote.T @ dZ2\n",
        "      db1 = dZ2.sum(axis=0)\n",
        "\n",
        "      ####################################\n",
        "      # IMPORTANTE\n",
        "      # la actualización de los parámetros\n",
        "      # debe hacerse de forma simultánea\n",
        "      W4 -= alpha * dW4; b4 -= alpha * db4\n",
        "      W3 -= alpha * dW3; b3 -= alpha * db3\n",
        "      W2 -= alpha * dW2; b2 -= alpha * db2\n",
        "      W1 -= alpha * dW1; b1 -= alpha * db1\n",
        "\n",
        "      # calcula la pérdida en la época\n",
        "      perdidas[i] += entropia_cruzada_binaria(ylote, y_hat)\n",
        "      exactitudes[i] += exactitud(ylote, np.round(y_hat))\n",
        "\n",
        "    perdidas[i] /= n_lotes\n",
        "    exactitudes[i] /= n_lotes\n",
        "    print(f'Epoch {i}: Pérdida = {perdidas[i]} Exactitud = {exactitudes[i]}')\n",
        "\n",
        "  return W1, b1, W2, b2, W3, b3, W4, b4, perdidas, exactitudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "W1, b1, W2, b2, W3, b3, W4, b4, perdidas, exactitudes = retropropagacion_nr_snp(Xent,\n",
        "                                                         yent[:, np.newaxis],\n",
        "                                                         alpha = 0.01,\n",
        "                                                         t_lote=16,\n",
        "                                                         n_epocas = 500,\n",
        "                                                         n_ocultas = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.arange(perdidas.size), perdidas, label='ECB')\n",
        "plt.plot(np.arange(exactitudes.size), exactitudes, label='Exactitud')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
